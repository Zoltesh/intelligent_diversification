{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52b25605",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zoltesh/projects/intelligent_diversification/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timezone\n",
        "import feature_engineering.technical_indicators as ti\n",
        "import kagglehub\n",
        "import polars as pl\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266442f3",
      "metadata": {},
      "source": [
        "# Cleaning and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ac78d7",
      "metadata": {},
      "source": [
        "### Download Parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6fbee06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching latest data from Kaggle: braydenmcarthur/10x-crypto-ohlcv-2024-2025...\n",
            "Downloading to /home/zoltesh/.cache/kagglehub/datasets/braydenmcarthur/10x-crypto-ohlcv-2024-2025/3.archive...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38.7M/38.7M [00:00<00:00, 83.2MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syncing 10 files to /home/zoltesh/projects/intelligent_diversification/src/data...\n",
            " -> Updated DOGE-USDC.parquet\n",
            " -> Updated XRP-USDC.parquet\n",
            " -> Updated BTC-USDC.parquet\n",
            " -> Updated BCH-USDC.parquet\n",
            " -> Updated LINK-USDC.parquet\n",
            " -> Updated ADA-USDC.parquet\n",
            " -> Updated LTC-USDC.parquet\n",
            " -> Updated ETH-USDC.parquet\n",
            " -> Updated SOL-USDC.parquet\n",
            " -> Updated AVAX-USDC.parquet\n",
            "\n",
            "Sync Complete. Files are located in: /home/zoltesh/projects/intelligent_diversification/src/data\n"
          ]
        }
      ],
      "source": [
        "def sync_and_save_parquet(dataset_slug: str, target_dirname: str = \"data\"):\n",
        "    # 1. Load credentials from .env\n",
        "    load_dotenv()\n",
        "    \n",
        "    if not os.getenv(\"KAGGLE_USERNAME\") or not os.getenv(\"KAGGLE_KEY\"):\n",
        "        raise EnvironmentError(\"KAGGLE_USERNAME or KAGGLE_KEY not found in .env file.\")\n",
        "\n",
        "    # 2. Define data path relative to where the code is running\n",
        "    project_root = Path.cwd()\n",
        "    local_data_dir = project_root / target_dirname\n",
        "    local_data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # 3. Download the dataset (Bypassing local cache)\n",
        "    print(f\"Fetching latest data from Kaggle: {dataset_slug}...\")\n",
        "    # force_download=True ensures kagglehub checks for the latest version and re-downloads\n",
        "    cache_path = Path(kagglehub.dataset_download(dataset_slug, force_download=True))\n",
        "\n",
        "    # 4. Find all parquet files in the fresh download\n",
        "    parquet_files = list(cache_path.glob(\"*.parquet\"))\n",
        "    \n",
        "    if not parquet_files:\n",
        "        print(\"No parquet files found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # 5. Move/Save to the local data folder\n",
        "    print(f\"Syncing {len(parquet_files)} files to {local_data_dir}...\")\n",
        "    for file in parquet_files:\n",
        "        destination = local_data_dir / file.name\n",
        "        \n",
        "        # Using shutil.copy2 to preserve metadata and save CPU/Memory \n",
        "        # instead of reading/writing via Polars\n",
        "        shutil.copy2(file, destination)\n",
        "        print(f\" -> Updated {file.name}\")\n",
        "\n",
        "    print(f\"\\nSync Complete. Files are located in: {local_data_dir}\")\n",
        "\n",
        "# Execution\n",
        "# Using your specific dataset slug\n",
        "DATASET_SLUG = \"braydenmcarthur/10x-crypto-ohlcv-2024-2025\"\n",
        "\n",
        "try:\n",
        "    sync_and_save_parquet(DATASET_SLUG)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169c3879",
      "metadata": {},
      "source": [
        "### Create the lazyframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c5fe6f89",
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_lf = pl.scan_parquet('data/ADA-USDC.parquet')\n",
        "avax_lf = pl.scan_parquet('data/AVAX-USDC.parquet')\n",
        "bch_lf = pl.scan_parquet('data/BCH-USDC.parquet')\n",
        "btc_lf = pl.scan_parquet('data/BTC-USDC.parquet')\n",
        "doge_lf = pl.scan_parquet('data/DOGE-USDC.parquet')\n",
        "eth_lf = pl.scan_parquet('data/ETH-USDC.parquet')\n",
        "link_lf = pl.scan_parquet('data/LINK-USDC.parquet')\n",
        "ltc_lf = pl.scan_parquet('data/LTC-USDC.parquet')\n",
        "sol_lf = pl.scan_parquet('data/SOL-USDC.parquet')\n",
        "xrp_lf = pl.scan_parquet('data/XRP-USDC.parquet')\n",
        "\n",
        "all_lf = {\n",
        "    'ADA': ada_lf,\n",
        "    'AVAX': avax_lf,\n",
        "    'BCH': bch_lf,\n",
        "    'BTC': btc_lf,\n",
        "    'DOGE': doge_lf,\n",
        "    'ETH': eth_lf,\n",
        "    'LINK': link_lf,\n",
        "    'LTC': ltc_lf,\n",
        "    'SOL': sol_lf,\n",
        "    'XRP': xrp_lf\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c4b083",
      "metadata": {},
      "source": [
        "### Identify sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e3aca6bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA: 0.06% missing\n",
            "AVAX: 0.08% missing\n",
            "BCH: 0.06% missing\n",
            "BTC: 0.04% missing\n",
            "DOGE: 0.05% missing\n",
            "ETH: 0.05% missing\n",
            "LINK: 0.05% missing\n",
            "LTC: 0.05% missing\n",
            "SOL: 0.05% missing\n",
            "XRP: 0.05% missing\n",
            "Total sparsity: 0.05%\n"
          ]
        }
      ],
      "source": [
        "# Print percentage missing out of 210,528 rows per file\n",
        "# Print percentage missing out of total rows\n",
        "total_rows = 0\n",
        "total_missing = 0\n",
        "for symbol, lf in all_lf.items():\n",
        "    missing = (210_528 - lf.collect().shape[0]) / 210_528 * 100\n",
        "    total_missing += missing\n",
        "    print(f\"{symbol}: {missing:.2f}% missing\")\n",
        "    total_rows += lf.collect().shape[0]\n",
        "\n",
        "total_sparsity = (2_105_280 - total_rows) / 2_105_280 * 100\n",
        "print(f\"Total sparsity: {total_sparsity:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7ea3fc37",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2104132\n"
          ]
        }
      ],
      "source": [
        "total = 0\n",
        "for symbol, lf in all_lf.items():\n",
        "    total += lf.collect().shape[0]\n",
        "print(total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0127abcc",
      "metadata": {},
      "source": [
        "# Detect any gaps/missing timestamp and forward fill them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fa7f4cbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA: imputed 121 rows -> data_cleaned/ADA-USDC.parquet\n",
            "AVAX: imputed 161 rows -> data_cleaned/AVAX-USDC.parquet\n",
            "BCH: imputed 129 rows -> data_cleaned/BCH-USDC.parquet\n",
            "BTC: imputed 94 rows -> data_cleaned/BTC-USDC.parquet\n",
            "DOGE: imputed 105 rows -> data_cleaned/DOGE-USDC.parquet\n",
            "ETH: imputed 101 rows -> data_cleaned/ETH-USDC.parquet\n",
            "LINK: imputed 106 rows -> data_cleaned/LINK-USDC.parquet\n",
            "LTC: imputed 107 rows -> data_cleaned/LTC-USDC.parquet\n",
            "SOL: imputed 112 rows -> data_cleaned/SOL-USDC.parquet\n",
            "XRP: imputed 112 rows -> data_cleaned/XRP-USDC.parquet\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = Path(\"data_cleaned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INTERVAL = \"5m\"\n",
        "PRICE_COLS = [\"open\", \"high\", \"low\", \"close\"]\n",
        "VOLUME_COLS = [\"volume\"]\n",
        "VALUE_COLS = PRICE_COLS + VOLUME_COLS\n",
        "\n",
        "def impute_gaps_ffill(lf: pl.LazyFrame, every: str = \"5m\") -> tuple[pl.LazyFrame, int]:\n",
        "    base = (\n",
        "        lf.select([\"timestamp\", *VALUE_COLS])\n",
        "          .with_columns(\n",
        "              pl.col(\"timestamp\").cast(pl.Datetime(\"ms\")),\n",
        "              *[pl.col(c).cast(pl.Float64) for c in VALUE_COLS],\n",
        "          )\n",
        "          .unique(subset=\"timestamp\", keep=\"first\")\n",
        "          .sort(\"timestamp\")\n",
        "    )\n",
        "\n",
        "    # derive start/end from the data (small collect)\n",
        "    bounds = base.select(\n",
        "        pl.col(\"timestamp\").min().alias(\"start\"),\n",
        "        pl.col(\"timestamp\").max().alias(\"end\"),\n",
        "    ).collect()\n",
        "    start = bounds[\"start\"][0]\n",
        "    end = bounds[\"end\"][0]\n",
        "\n",
        "    full_index = pl.datetime_range(\n",
        "        start, end, interval=every, time_unit=\"ms\", eager=True\n",
        "    )\n",
        "\n",
        "    skeleton = pl.DataFrame({\"timestamp\": full_index}).lazy()\n",
        "\n",
        "    joined = skeleton.join(base, on=\"timestamp\", how=\"left\")\n",
        "\n",
        "    imputed_count = int(\n",
        "        joined.select(pl.col(\"open\").is_null().sum()).collect().item()\n",
        "    )\n",
        "\n",
        "    out = (\n",
        "        joined.with_columns(\n",
        "            pl.col(VALUE_COLS).fill_null(strategy=\"forward\")  # includes volume now\n",
        "        )\n",
        "        .with_columns(pl.col(\"timestamp\").dt.timestamp(\"ms\").cast(pl.Int64()))\n",
        "        .select([\"timestamp\", *VALUE_COLS])\n",
        "    )\n",
        "\n",
        "    return out, imputed_count\n",
        "\n",
        "for symbol, lf in all_lf.items():\n",
        "    out_lf, imputed = impute_gaps_ffill(lf, every=INTERVAL)\n",
        "    out_path = OUT_DIR / f\"{symbol}-USDC.parquet\"\n",
        "    out_lf.sink_parquet(out_path)\n",
        "    print(f\"{symbol}: imputed {imputed} rows -> {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e735e484",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File                 |     Rows | DupTS | Gaps | Nulls | Status\n",
            "----------------------------------------------------------------------\n",
            "ADA-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "AVAX-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "BCH-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "BTC-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "DOGE-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "ETH-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "LINK-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "LTC-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "SOL-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "XRP-USDC.parquet     |   210528 | False | False |     0 | OK\n"
          ]
        }
      ],
      "source": [
        "CLEANED_DIR = Path(\"data_cleaned\")\n",
        "STEP_MS = 300_000  # 5 minutes\n",
        "\n",
        "print(f\"{'File':<20} | {'Rows':>8} | {'DupTS':>5} | {'Gaps':>4} | {'Nulls':>5} | {'Status'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for path in sorted(CLEANED_DIR.glob(\"*.parquet\")):\n",
        "    df = pl.read_parquet(path).select([\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]).sort(\"timestamp\")\n",
        "\n",
        "    ts = df[\"timestamp\"]\n",
        "    d = ts.diff().drop_nulls()\n",
        "\n",
        "    has_gaps = not (d == STEP_MS).all()\n",
        "    has_dups = ts.n_unique() != ts.len()\n",
        "\n",
        "    # expected row count for a perfectly continuous series over [min, max]\n",
        "    expected_rows = int((ts.max() - ts.min()) // STEP_MS + 1)\n",
        "    wrong_count = df.height != expected_rows\n",
        "\n",
        "    nulls = int(df.null_count().sum_horizontal().item())\n",
        "\n",
        "    status = \"OK\" if (nulls == 0 and not has_gaps and not has_dups and not wrong_count) else \"ERROR\"\n",
        "    print(f\"{path.name:<20} | {df.height:>8} | {str(has_dups):>5} | {str(has_gaps):>4} | {nulls:>5} | {status}\")\n",
        "\n",
        "    if status == \"ERROR\":\n",
        "        # show the first few offending deltas\n",
        "        bad = df.select(\n",
        "            pl.col(\"timestamp\"),\n",
        "            pl.col(\"timestamp\").diff().alias(\"dt\"),\n",
        "        ).filter(pl.col(\"dt\").is_not_null() & (pl.col(\"dt\") != STEP_MS)).head(10)\n",
        "        if bad.height:\n",
        "            print(bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc0dd60",
      "metadata": {},
      "source": [
        "# Create Cleaned LazyFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3217ae6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_lf_cleaned = pl.scan_parquet('data_cleaned/ADA-USDC.parquet')\n",
        "avax_lf_cleaned = pl.scan_parquet('data_cleaned/AVAX-USDC.parquet')\n",
        "bch_lf_cleaned = pl.scan_parquet('data_cleaned/BCH-USDC.parquet')\n",
        "btc_lf_cleaned = pl.scan_parquet('data_cleaned/BTC-USDC.parquet')\n",
        "doge_lf_cleaned = pl.scan_parquet('data_cleaned/DOGE-USDC.parquet')\n",
        "eth_lf_cleaned = pl.scan_parquet('data_cleaned/ETH-USDC.parquet')\n",
        "link_lf_cleaned = pl.scan_parquet('data_cleaned/LINK-USDC.parquet')\n",
        "ltc_lf_cleaned = pl.scan_parquet('data_cleaned/LTC-USDC.parquet')\n",
        "sol_lf_cleaned = pl.scan_parquet('data_cleaned/SOL-USDC.parquet')\n",
        "xrp_lf_cleaned = pl.scan_parquet('data_cleaned/XRP-USDC.parquet')\n",
        "\n",
        "all_lf_cleaned = {\n",
        "    'ADA': ada_lf_cleaned,\n",
        "    'AVAX': avax_lf_cleaned,\n",
        "    'BCH': bch_lf_cleaned,\n",
        "    'BTC': btc_lf_cleaned,\n",
        "    'DOGE': doge_lf_cleaned,\n",
        "    'ETH': eth_lf_cleaned,\n",
        "    'LINK': link_lf_cleaned,\n",
        "    'LTC': ltc_lf_cleaned,\n",
        "    'SOL': sol_lf_cleaned,\n",
        "    'XRP': xrp_lf_cleaned\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "346cf989",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "AVAX 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "BCH 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "BTC 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "DOGE 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "ETH 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "LINK 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "LTC 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "SOL 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "XRP 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n"
          ]
        }
      ],
      "source": [
        "# Print min and max timestamps for each symbol\n",
        "for symbol, lf in all_lf_cleaned.items():\n",
        "    ts_min, ts_max = (\n",
        "        lf.select(\n",
        "            pl.from_epoch(\"timestamp\", time_unit=\"ms\").min().dt.replace_time_zone(\"UTC\").alias(\"min_dt_utc\"),\n",
        "            pl.from_epoch(\"timestamp\", time_unit=\"ms\").max().dt.replace_time_zone(\"UTC\").alias(\"max_dt_utc\"),\n",
        "        )\n",
        "        .collect()\n",
        "        .row(0)\n",
        "    )\n",
        "    print(f\"{symbol} {ts_min} {ts_max}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582b1c57",
      "metadata": {},
      "source": [
        "# Technical Indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6050b7f5",
      "metadata": {},
      "source": [
        "### RSI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b1c9a5a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "new_df = ti.add_atr(df=btc_lf_cleaned.collect(), tf='15m', period=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5c38e1f4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (100, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>open</th><th>high</th><th>low</th><th>close</th><th>volume</th><th>atr_14_15m</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1752547800000</td><td>118295.96</td><td>118414.72</td><td>118228.98</td><td>118382.33</td><td>18.311497</td><td>524.665625</td></tr><tr><td>1752552000000</td><td>117148.03</td><td>117210.02</td><td>117000.0</td><td>117003.81</td><td>60.289319</td><td>545.086205</td></tr><tr><td>1752564300000</td><td>117000.0</td><td>117057.39</td><td>116899.99</td><td>116900.0</td><td>106.246233</td><td>535.855581</td></tr><tr><td>1752584400000</td><td>117179.93</td><td>117311.7</td><td>117133.38</td><td>117311.7</td><td>48.193969</td><td>532.899496</td></tr><tr><td>1752592800000</td><td>116220.32</td><td>116220.32</td><td>115840.31</td><td>115915.81</td><td>143.871487</td><td>555.244508</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1753367700000</td><td>118508.16</td><td>118775.99</td><td>118469.69</td><td>118765.41</td><td>49.183092</td><td>567.936498</td></tr><tr><td>1753379100000</td><td>119258.14</td><td>119301.48</td><td>119249.81</td><td>119286.36</td><td>16.155273</td><td>567.177772</td></tr><tr><td>1753391700000</td><td>118650.37</td><td>118672.6</td><td>118629.0</td><td>118630.86</td><td>17.802014</td><td>569.324968</td></tr><tr><td>1753399200000</td><td>118478.07</td><td>118478.07</td><td>118388.16</td><td>118422.12</td><td>37.764929</td><td>561.548183</td></tr><tr><td>1753404000000</td><td>117750.0</td><td>117852.0</td><td>117678.6</td><td>117846.53</td><td>102.596598</td><td>565.880845</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (100, 7)\n",
              "┌───────────────┬───────────┬───────────┬───────────┬───────────┬────────────┬────────────┐\n",
              "│ timestamp     ┆ open      ┆ high      ┆ low       ┆ close     ┆ volume     ┆ atr_14_15m │\n",
              "│ ---           ┆ ---       ┆ ---       ┆ ---       ┆ ---       ┆ ---        ┆ ---        │\n",
              "│ i64           ┆ f64       ┆ f64       ┆ f64       ┆ f64       ┆ f64        ┆ f64        │\n",
              "╞═══════════════╪═══════════╪═══════════╪═══════════╪═══════════╪════════════╪════════════╡\n",
              "│ 1752547800000 ┆ 118295.96 ┆ 118414.72 ┆ 118228.98 ┆ 118382.33 ┆ 18.311497  ┆ 524.665625 │\n",
              "│ 1752552000000 ┆ 117148.03 ┆ 117210.02 ┆ 117000.0  ┆ 117003.81 ┆ 60.289319  ┆ 545.086205 │\n",
              "│ 1752564300000 ┆ 117000.0  ┆ 117057.39 ┆ 116899.99 ┆ 116900.0  ┆ 106.246233 ┆ 535.855581 │\n",
              "│ 1752584400000 ┆ 117179.93 ┆ 117311.7  ┆ 117133.38 ┆ 117311.7  ┆ 48.193969  ┆ 532.899496 │\n",
              "│ 1752592800000 ┆ 116220.32 ┆ 116220.32 ┆ 115840.31 ┆ 115915.81 ┆ 143.871487 ┆ 555.244508 │\n",
              "│ …             ┆ …         ┆ …         ┆ …         ┆ …         ┆ …          ┆ …          │\n",
              "│ 1753367700000 ┆ 118508.16 ┆ 118775.99 ┆ 118469.69 ┆ 118765.41 ┆ 49.183092  ┆ 567.936498 │\n",
              "│ 1753379100000 ┆ 119258.14 ┆ 119301.48 ┆ 119249.81 ┆ 119286.36 ┆ 16.155273  ┆ 567.177772 │\n",
              "│ 1753391700000 ┆ 118650.37 ┆ 118672.6  ┆ 118629.0  ┆ 118630.86 ┆ 17.802014  ┆ 569.324968 │\n",
              "│ 1753399200000 ┆ 118478.07 ┆ 118478.07 ┆ 118388.16 ┆ 118422.12 ┆ 37.764929  ┆ 561.548183 │\n",
              "│ 1753404000000 ┆ 117750.0  ┆ 117852.0  ┆ 117678.6  ┆ 117846.53 ┆ 102.596598 ┆ 565.880845 │\n",
              "└───────────────┴───────────┴───────────┴───────────┴───────────┴────────────┴────────────┘"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df.tail(100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
