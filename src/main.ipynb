{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "52b25605",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import gc\n",
        "import feature_engineering.technical_indicators as ti\n",
        "import kagglehub\n",
        "import polars as pl\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "from utils.vif import remove_high_vif"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266442f3",
      "metadata": {},
      "source": [
        "# Cleaning and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ac78d7",
      "metadata": {},
      "source": [
        "### Download Parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c6fbee06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching latest data from Kaggle: braydenmcarthur/10x-crypto-ohlcv-2024-2025...\n",
            "Downloading to /home/zoltesh/.cache/kagglehub/datasets/braydenmcarthur/10x-crypto-ohlcv-2024-2025/3.archive...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38.7M/38.7M [00:00<00:00, 78.0MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Syncing 10 files to /home/zoltesh/projects/intelligent_diversification/src/data...\n",
            " -> Updated DOGE-USDC.parquet\n",
            " -> Updated XRP-USDC.parquet\n",
            " -> Updated BTC-USDC.parquet\n",
            " -> Updated BCH-USDC.parquet\n",
            " -> Updated LINK-USDC.parquet\n",
            " -> Updated ADA-USDC.parquet\n",
            " -> Updated LTC-USDC.parquet\n",
            " -> Updated ETH-USDC.parquet\n",
            " -> Updated SOL-USDC.parquet\n",
            " -> Updated AVAX-USDC.parquet\n",
            "\n",
            "Sync Complete. Files are located in: /home/zoltesh/projects/intelligent_diversification/src/data\n"
          ]
        }
      ],
      "source": [
        "def sync_and_save_parquet(dataset_slug: str, target_dirname: str = \"data\"):\n",
        "    # 1. Load credentials from .env\n",
        "    load_dotenv()\n",
        "    \n",
        "    if not os.getenv(\"KAGGLE_USERNAME\") or not os.getenv(\"KAGGLE_KEY\"):\n",
        "        raise EnvironmentError(\"KAGGLE_USERNAME or KAGGLE_KEY not found in .env file.\")\n",
        "\n",
        "    # 2. Define data path relative to where the code is running\n",
        "    project_root = Path.cwd()\n",
        "    local_data_dir = project_root / target_dirname\n",
        "    local_data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # 3. Download the dataset (Bypassing local cache)\n",
        "    print(f\"Fetching latest data from Kaggle: {dataset_slug}...\")\n",
        "    # force_download=True ensures kagglehub checks for the latest version and re-downloads\n",
        "    cache_path = Path(kagglehub.dataset_download(dataset_slug, force_download=True))\n",
        "\n",
        "    # 4. Find all parquet files in the fresh download\n",
        "    parquet_files = list(cache_path.glob(\"*.parquet\"))\n",
        "    \n",
        "    if not parquet_files:\n",
        "        print(\"No parquet files found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # 5. Move/Save to the local data folder\n",
        "    print(f\"Syncing {len(parquet_files)} files to {local_data_dir}...\")\n",
        "    for file in parquet_files:\n",
        "        destination = local_data_dir / file.name\n",
        "        \n",
        "        # Using shutil.copy2 to preserve metadata and save CPU/Memory \n",
        "        # instead of reading/writing via Polars\n",
        "        shutil.copy2(file, destination)\n",
        "        print(f\" -> Updated {file.name}\")\n",
        "\n",
        "    print(f\"\\nSync Complete. Files are located in: {local_data_dir}\")\n",
        "\n",
        "# Execution\n",
        "# Using your specific dataset slug\n",
        "DATASET_SLUG = \"braydenmcarthur/10x-crypto-ohlcv-2024-2025\"\n",
        "\n",
        "try:\n",
        "    sync_and_save_parquet(DATASET_SLUG)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169c3879",
      "metadata": {},
      "source": [
        "### Create the lazyframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c5fe6f89",
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_lf = pl.scan_parquet('data/ADA-USDC.parquet')\n",
        "avax_lf = pl.scan_parquet('data/AVAX-USDC.parquet')\n",
        "bch_lf = pl.scan_parquet('data/BCH-USDC.parquet')\n",
        "btc_lf = pl.scan_parquet('data/BTC-USDC.parquet')\n",
        "doge_lf = pl.scan_parquet('data/DOGE-USDC.parquet')\n",
        "eth_lf = pl.scan_parquet('data/ETH-USDC.parquet')\n",
        "link_lf = pl.scan_parquet('data/LINK-USDC.parquet')\n",
        "ltc_lf = pl.scan_parquet('data/LTC-USDC.parquet')\n",
        "sol_lf = pl.scan_parquet('data/SOL-USDC.parquet')\n",
        "xrp_lf = pl.scan_parquet('data/XRP-USDC.parquet')\n",
        "\n",
        "all_lf = {\n",
        "    'ADA': ada_lf,\n",
        "    'AVAX': avax_lf,\n",
        "    'BCH': bch_lf,\n",
        "    'BTC': btc_lf,\n",
        "    'DOGE': doge_lf,\n",
        "    'ETH': eth_lf,\n",
        "    'LINK': link_lf,\n",
        "    'LTC': ltc_lf,\n",
        "    'SOL': sol_lf,\n",
        "    'XRP': xrp_lf\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c4b083",
      "metadata": {},
      "source": [
        "### Identify sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e3aca6bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA: 0.06% missing\n",
            "AVAX: 0.08% missing\n",
            "BCH: 0.06% missing\n",
            "BTC: 0.04% missing\n",
            "DOGE: 0.05% missing\n",
            "ETH: 0.05% missing\n",
            "LINK: 0.05% missing\n",
            "LTC: 0.05% missing\n",
            "SOL: 0.05% missing\n",
            "XRP: 0.05% missing\n",
            "Total sparsity: 0.05%\n"
          ]
        }
      ],
      "source": [
        "# Print percentage missing out of 210,528 rows per file\n",
        "# Print percentage missing out of total rows\n",
        "total_rows = 0\n",
        "total_missing = 0\n",
        "for symbol, lf in all_lf.items():\n",
        "    rows = lf.select(pl.len()).collect().item()\n",
        "    missing = (210_528 - rows) / 210_528 * 100\n",
        "    total_missing += missing\n",
        "    print(f\"{symbol}: {missing:.2f}% missing\")\n",
        "    total_rows += rows\n",
        "\n",
        "total_sparsity = (2_105_280 - total_rows) / 2_105_280 * 100\n",
        "print(f\"Total sparsity: {total_sparsity:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7ea3fc37",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2104132\n"
          ]
        }
      ],
      "source": [
        "total = 0\n",
        "for symbol, lf in all_lf.items():\n",
        "    total += lf.collect().shape[0]\n",
        "print(total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0127abcc",
      "metadata": {},
      "source": [
        "# Detect any gaps/missing timestamp and forward fill them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fa7f4cbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA: imputed 121 rows -> data_cleaned/ADA-USDC.parquet\n",
            "AVAX: imputed 161 rows -> data_cleaned/AVAX-USDC.parquet\n",
            "BCH: imputed 129 rows -> data_cleaned/BCH-USDC.parquet\n",
            "BTC: imputed 94 rows -> data_cleaned/BTC-USDC.parquet\n",
            "DOGE: imputed 105 rows -> data_cleaned/DOGE-USDC.parquet\n",
            "ETH: imputed 101 rows -> data_cleaned/ETH-USDC.parquet\n",
            "LINK: imputed 106 rows -> data_cleaned/LINK-USDC.parquet\n",
            "LTC: imputed 107 rows -> data_cleaned/LTC-USDC.parquet\n",
            "SOL: imputed 112 rows -> data_cleaned/SOL-USDC.parquet\n",
            "XRP: imputed 112 rows -> data_cleaned/XRP-USDC.parquet\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = Path(\"data_cleaned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INTERVAL = \"5m\"\n",
        "PRICE_COLS = [\"open\", \"high\", \"low\", \"close\"]\n",
        "VOLUME_COLS = [\"volume\"]\n",
        "VALUE_COLS = PRICE_COLS + VOLUME_COLS\n",
        "\n",
        "def impute_gaps_ffill(lf: pl.LazyFrame, every: str = \"5m\") -> tuple[pl.LazyFrame, int]:\n",
        "    base = (\n",
        "        lf.select([\"timestamp\", *VALUE_COLS])\n",
        "          .with_columns(\n",
        "              pl.col(\"timestamp\").cast(pl.Datetime(\"ms\")),\n",
        "              *[pl.col(c).cast(pl.Float64) for c in VALUE_COLS],\n",
        "          )\n",
        "          .unique(subset=\"timestamp\", keep=\"first\")\n",
        "          .sort(\"timestamp\")\n",
        "    )\n",
        "\n",
        "    # derive start/end from the data (small collect)\n",
        "    bounds = base.select(\n",
        "        pl.col(\"timestamp\").min().alias(\"start\"),\n",
        "        pl.col(\"timestamp\").max().alias(\"end\"),\n",
        "    ).collect()\n",
        "    start = bounds[\"start\"][0]\n",
        "    end = bounds[\"end\"][0]\n",
        "\n",
        "    full_index = pl.datetime_range(\n",
        "        start, end, interval=every, time_unit=\"ms\", eager=True\n",
        "    )\n",
        "\n",
        "    skeleton = pl.DataFrame({\"timestamp\": full_index}).lazy()\n",
        "\n",
        "    joined = skeleton.join(base, on=\"timestamp\", how=\"left\")\n",
        "\n",
        "    imputed_count = int(\n",
        "        joined.select(pl.col(\"open\").is_null().sum()).collect().item()\n",
        "    )\n",
        "\n",
        "    out = (\n",
        "        joined.with_columns(\n",
        "            pl.col(VALUE_COLS).fill_null(strategy=\"forward\")  # includes volume now\n",
        "        )\n",
        "        .with_columns(pl.col(\"timestamp\").dt.timestamp(\"ms\").cast(pl.Int64()))\n",
        "        .select([\"timestamp\", *VALUE_COLS])\n",
        "    )\n",
        "\n",
        "    return out, imputed_count\n",
        "\n",
        "for symbol, lf in all_lf.items():\n",
        "    out_lf, imputed = impute_gaps_ffill(lf, every=INTERVAL)\n",
        "    out_path = OUT_DIR / f\"{symbol}-USDC.parquet\"\n",
        "    out_lf.sink_parquet(out_path)\n",
        "    print(f\"{symbol}: imputed {imputed} rows -> {out_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309e3571",
      "metadata": {},
      "source": [
        "### Ensure:\n",
        "### - Rows = 210528\n",
        "### - No Duplicates\n",
        "### - No Gaps\n",
        "### - No Nulls\n",
        "### - Status OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e735e484",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File                 |     Rows | DupTS | Gaps | Nulls | Status\n",
            "----------------------------------------------------------------------\n",
            "ADA-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "AVAX-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "BCH-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "BTC-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "DOGE-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "ETH-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "LINK-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "LTC-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "SOL-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "XRP-USDC.parquet     |   210528 | False | False |     0 | OK\n"
          ]
        }
      ],
      "source": [
        "CLEANED_DIR = Path(\"data_cleaned\")\n",
        "STEP_MS = 300_000  # 5 minutes\n",
        "\n",
        "print(f\"{'File':<20} | {'Rows':>8} | {'DupTS':>5} | {'Gaps':>4} | {'Nulls':>5} | {'Status'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for path in sorted(CLEANED_DIR.glob(\"*.parquet\")):\n",
        "    df = pl.read_parquet(path).select([\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]).sort(\"timestamp\")\n",
        "\n",
        "    ts = df[\"timestamp\"]\n",
        "    d = ts.diff().drop_nulls()\n",
        "\n",
        "    has_gaps = not (d == STEP_MS).all()\n",
        "    has_dups = ts.n_unique() != ts.len()\n",
        "\n",
        "    # expected row count for a perfectly continuous series over [min, max]\n",
        "    expected_rows = int((ts.max() - ts.min()) // STEP_MS + 1)\n",
        "    wrong_count = df.height != expected_rows\n",
        "\n",
        "    nulls = int(df.null_count().sum_horizontal().item())\n",
        "\n",
        "    status = \"OK\" if (nulls == 0 and not has_gaps and not has_dups and not wrong_count) else \"ERROR\"\n",
        "    print(f\"{path.name:<20} | {df.height:>8} | {str(has_dups):>5} | {str(has_gaps):>4} | {nulls:>5} | {status}\")\n",
        "\n",
        "    if status == \"ERROR\":\n",
        "        # show the first few offending deltas\n",
        "        bad = df.select(\n",
        "            pl.col(\"timestamp\"),\n",
        "            pl.col(\"timestamp\").diff().alias(\"dt\"),\n",
        "        ).filter(pl.col(\"dt\").is_not_null() & (pl.col(\"dt\") != STEP_MS)).head(10)\n",
        "        if bad.height:\n",
        "            print(bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc0dd60",
      "metadata": {},
      "source": [
        "# Create Cleaned LazyFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3217ae6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_lf_cleaned = pl.scan_parquet('data_cleaned/ADA-USDC.parquet')\n",
        "avax_lf_cleaned = pl.scan_parquet('data_cleaned/AVAX-USDC.parquet')\n",
        "bch_lf_cleaned = pl.scan_parquet('data_cleaned/BCH-USDC.parquet')\n",
        "btc_lf_cleaned = pl.scan_parquet('data_cleaned/BTC-USDC.parquet')\n",
        "doge_lf_cleaned = pl.scan_parquet('data_cleaned/DOGE-USDC.parquet')\n",
        "eth_lf_cleaned = pl.scan_parquet('data_cleaned/ETH-USDC.parquet')\n",
        "link_lf_cleaned = pl.scan_parquet('data_cleaned/LINK-USDC.parquet')\n",
        "ltc_lf_cleaned = pl.scan_parquet('data_cleaned/LTC-USDC.parquet')\n",
        "sol_lf_cleaned = pl.scan_parquet('data_cleaned/SOL-USDC.parquet')\n",
        "xrp_lf_cleaned = pl.scan_parquet('data_cleaned/XRP-USDC.parquet')\n",
        "\n",
        "all_lf_cleaned = {\n",
        "    'ADA': ada_lf_cleaned,\n",
        "    'AVAX': avax_lf_cleaned,\n",
        "    'BCH': bch_lf_cleaned,\n",
        "    'BTC': btc_lf_cleaned,\n",
        "    'DOGE': doge_lf_cleaned,\n",
        "    'ETH': eth_lf_cleaned,\n",
        "    'LINK': link_lf_cleaned,\n",
        "    'LTC': ltc_lf_cleaned,\n",
        "    'SOL': sol_lf_cleaned,\n",
        "    'XRP': xrp_lf_cleaned\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1b81ce8",
      "metadata": {},
      "source": [
        "### Ensure date range still intact (2024-01-01 00:00:00 to 2025-12-31 23:55:00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "346cf989",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "AVAX 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "BCH 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "BTC 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "DOGE 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "ETH 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "LINK 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "LTC 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "SOL 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n",
            "XRP 2024-01-01 00:00:00+00:00 2025-12-31 23:55:00+00:00\n"
          ]
        }
      ],
      "source": [
        "# Print min and max timestamps for each symbol\n",
        "for symbol, lf in all_lf_cleaned.items():\n",
        "    ts_min, ts_max = (\n",
        "        lf.select(\n",
        "            pl.from_epoch(\"timestamp\", time_unit=\"ms\").min().dt.replace_time_zone(\"UTC\").alias(\"min_dt_utc\"),\n",
        "            pl.from_epoch(\"timestamp\", time_unit=\"ms\").max().dt.replace_time_zone(\"UTC\").alias(\"max_dt_utc\"),\n",
        "        )\n",
        "        .collect()\n",
        "        .row(0)\n",
        "    )\n",
        "    print(f\"{symbol} {ts_min} {ts_max}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582b1c57",
      "metadata": {},
      "source": [
        "# Technical Indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6050b7f5",
      "metadata": {},
      "source": [
        "### Create Feature Engineered Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b218f826",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_825/339006250.py:22: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
            "  symbol: engineer_features(lf.collect(streaming=True), tfs, periods)\n"
          ]
        }
      ],
      "source": [
        "tfs = ['15m', '30m', '1h']\n",
        "periods = [7, 14, 20, 25, 30]\n",
        "\n",
        "SPECIAL_INDICATORS = [\n",
        "    (\"obv\", dict()),\n",
        "    (\"macd\", dict(fastperiod=12, slowperiod=26, signalperiod=9)),\n",
        "    (\"bbands\", dict(period=20, nbdevup=2.0, nbdevdn=2.0, matype=0)),\n",
        "]\n",
        "\n",
        "def engineer_features(df, tfs, periods):\n",
        "    for tf in tfs:\n",
        "        for period in periods:\n",
        "            df = ti.add_indicators(df=df, tf=tf, period=period)\n",
        "\n",
        "        for name, kwargs in SPECIAL_INDICATORS:\n",
        "            func = getattr(ti, f\"add_{name}\")\n",
        "            df = func(df=df, tf=tf, **kwargs)\n",
        "\n",
        "    return df\n",
        "\n",
        "feature_dfs = {\n",
        "    symbol: engineer_features(lf.collect(streaming=True), tfs, periods)\n",
        "    for symbol, lf in all_lf_cleaned.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23fbd3d0",
      "metadata": {},
      "source": [
        "### Identify features containing VIF less than max_vif (default 10.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c31d0563",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ADA\n",
            "Processing AVAX\n",
            "Processing BCH\n",
            "Processing BTC\n",
            "Processing DOGE\n",
            "Processing ETH\n",
            "Processing LINK\n",
            "Processing LTC\n",
            "Processing SOL\n",
            "Processing XRP\n"
          ]
        }
      ],
      "source": [
        "# Iteratively remove features with high VIF for each feature df\n",
        "vif_dfs = {\n",
        "    symbol: remove_high_vif(feature_df, max_vif=10.0, label=symbol)\n",
        "    for symbol, feature_df in feature_dfs.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "28dca08f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for symbol, vif_df in vif_dfs.items():\n",
        "    vif_df.write_csv(f'{symbol}_vif_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "42148388",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep only the features with VIF < 10 + core columns\n",
        "core_cols = [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
        "filtered_feature_dfs = {\n",
        "    symbol: feature_dfs[symbol].select(core_cols + vif_df[\"feature\"].to_list())\n",
        "    for symbol, vif_df in vif_dfs.items()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "91b68edd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# for symbol, filtered_df in filtered_feature_dfs.items():\n",
        "#     print(f'{symbol}: {filtered_df.columns}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "8f9230e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n",
            "(210528, 192)\n",
            "(209450, 192)\n"
          ]
        }
      ],
      "source": [
        "# Remove all rows with any NaNs or Nulls\n",
        "for symbol, df in feature_dfs.items():\n",
        "    print(df.shape)\n",
        "    df = df.filter(~pl.any_horizontal(pl.all().is_nan()))\n",
        "    print(df.shape)\n",
        "    feature_dfs[symbol] = df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "073796dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA has rows at timestamp 1735689600000\n",
            "AVAX has rows at timestamp 1735689600000\n",
            "BCH has rows at timestamp 1735689600000\n",
            "BTC has rows at timestamp 1735689600000\n",
            "DOGE has rows at timestamp 1735689600000\n",
            "ETH has rows at timestamp 1735689600000\n",
            "LINK has rows at timestamp 1735689600000\n",
            "LTC has rows at timestamp 1735689600000\n",
            "SOL has rows at timestamp 1735689600000\n",
            "XRP has rows at timestamp 1735689600000\n"
          ]
        }
      ],
      "source": [
        "for symbol, df in feature_dfs.items():\n",
        "    if df.filter(pl.col('timestamp') == 1735689600000).shape[0] != 0:\n",
        "        print(f'{symbol} has rows at timestamp 1735689600000')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "dbc74c9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create engineered_features directory\n",
        "os.makedirs('engineered_features', exist_ok=True)\n",
        "\n",
        "# Save all original feature dfs\n",
        "for symbol, df in feature_dfs.items():\n",
        "    df.write_csv(f'engineered_features/{symbol}_feature_df.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b3ab33bd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "498"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For each feature df, select only the columns from the corresponding vif_df + core columns\n",
        "for symbol, df in feature_dfs.items():\n",
        "    vif_df = vif_dfs[symbol]\n",
        "    df = df.select(core_cols + vif_df[\"feature\"].to_list())\n",
        "    df.write_csv(f'engineered_features/{symbol}_feature_df.csv')\n",
        "\n",
        "del feature_dfs, vif_dfs, filtered_feature_dfs\n",
        "gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
