{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52b25605",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zoltesh/projects/intelligent_diversification/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import kagglehub\n",
        "import polars as pl\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ac78d7",
      "metadata": {},
      "source": [
        "# Download Parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6fbee06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching latest data from Kaggle: braydenmcarthur/10x-crypto-ohlcv-2024-2025...\n",
            "Saving 10 files to /home/zoltesh/projects/intelligent_diversification/src/data...\n",
            " -> Saved DOGE-USDC.parquet\n",
            " -> Saved XRP-USDC.parquet\n",
            " -> Saved BTC-USDC.parquet\n",
            " -> Saved BCH-USDC.parquet\n",
            " -> Saved LINK-USDC.parquet\n",
            " -> Saved ADA-USDC.parquet\n",
            " -> Saved LTC-USDC.parquet\n",
            " -> Saved ETH-USDC.parquet\n",
            " -> Saved SOL-USDC.parquet\n",
            " -> Saved AVAX-USDC.parquet\n",
            "\n",
            "Sync Complete. Files are located in: /home/zoltesh/projects/intelligent_diversification/src/data\n"
          ]
        }
      ],
      "source": [
        "def sync_and_save_parquet(dataset_slug: str, target_dirname: str = \"data\"):\n",
        "    # 1. Load credentials from .env\n",
        "    load_dotenv()\n",
        "    \n",
        "    if not os.getenv(\"KAGGLE_USERNAME\") or not os.getenv(\"KAGGLE_KEY\"):\n",
        "        raise EnvironmentError(\"KAGGLE_USERNAME or KAGGLE_KEY not found in .env file.\")\n",
        "\n",
        "    # 2. Define data path relative to where the code is running\n",
        "    # This works in Notebooks and .py scripts\n",
        "    project_root = Path.cwd()\n",
        "    local_data_dir = project_root / target_dirname\n",
        "    local_data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # 3. Download the dataset (returns path to kagglehub cache)\n",
        "    print(f\"Fetching latest data from Kaggle: {dataset_slug}...\")\n",
        "    cache_path = Path(kagglehub.dataset_download(dataset_slug))\n",
        "\n",
        "    # 4. Find all parquet files\n",
        "    parquet_files = list(cache_path.glob(\"*.parquet\"))\n",
        "    \n",
        "    if not parquet_files:\n",
        "        print(\"No parquet files found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # 5. Save to the local data folder\n",
        "    print(f\"Saving {len(parquet_files)} files to {local_data_dir}...\")\n",
        "    for file in parquet_files:\n",
        "        # Load and write using Polars\n",
        "        lf = pl.scan_parquet(file)\n",
        "        destination = local_data_dir / file.name\n",
        "        \n",
        "        # We use collect() here to pull the data from cache and write it locally\n",
        "        lf.collect().write_parquet(destination)\n",
        "        print(f\" -> Saved {file.name}\")\n",
        "\n",
        "    print(f\"\\nSync Complete. Files are located in: {local_data_dir}\")\n",
        "\n",
        "# Execution\n",
        "DATASET_SLUG = \"braydenmcarthur/10x-crypto-ohlcv-2024-2025\"\n",
        "\n",
        "try:\n",
        "    sync_and_save_parquet(DATASET_SLUG)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c4b083",
      "metadata": {},
      "source": [
        "# Check Total Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e3aca6bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of records: 2104131\n"
          ]
        }
      ],
      "source": [
        "num_records = 0\n",
        "for file in os.listdir('data/'):\n",
        "    num_records += pl.scan_parquet(f'data/{file}').collect().shape[0]\n",
        "print(f\"Total number of records: {num_records}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fa7f4cbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ADA-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/ADA-USDC.parquet\n",
            "Processing AVAX-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/AVAX-USDC.parquet\n",
            "Processing BCH-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/BCH-USDC.parquet\n",
            "Processing BTC-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/BTC-USDC.parquet\n",
            "Processing DOGE-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/DOGE-USDC.parquet\n",
            "Processing ETH-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/ETH-USDC.parquet\n",
            "Processing LINK-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/LINK-USDC.parquet\n",
            "Processing LTC-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/LTC-USDC.parquet\n",
            "Processing SOL-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/SOL-USDC.parquet\n",
            "Processing XRP-USDC.parquet...\n",
            "Successfully saved to /home/zoltesh/projects/intelligent_diversification/data_cleaned/XRP-USDC.parquet\n",
            "\n",
            "All files processed successfully.\n"
          ]
        }
      ],
      "source": [
        "# --- Path Resolution ---\n",
        "CWD = Path.cwd()\n",
        "DATA_DIR_CANDIDATES = [\n",
        "    CWD / \"src\" / \"data\",\n",
        "    CWD / \"data\",\n",
        "    CWD.parent / \"src\" / \"data\",\n",
        "    CWD.parent / \"data\",\n",
        "]\n",
        "DATA_DIR = next((p for p in DATA_DIR_CANDIDATES if p.is_dir()), None)\n",
        "\n",
        "if DATA_DIR is None:\n",
        "    raise FileNotFoundError(\"Could not locate data directory.\")\n",
        "\n",
        "PROJECT_ROOT = DATA_DIR.parent if DATA_DIR.parent.name != \"src\" else DATA_DIR.parent.parent\n",
        "OUT_DIR = PROJECT_ROOT / \"data_cleaned\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Constants & Config ---\n",
        "START_MS = 1_704_067_200_000  # 2024-01-01 00:00:00 UTC\n",
        "END_MS = 1_767_225_300_000    # 2025-12-31 23:55:00 UTC\n",
        "INTERVAL = \"5m\"\n",
        "EXPECTED_ROWS = 210_528\n",
        "PRICE_COLS = [\"open\", \"high\", \"low\", \"close\"]\n",
        "VOLUME_COL = [\"volume\"]\n",
        "VALUE_COLS = PRICE_COLS + VOLUME_COL\n",
        "\n",
        "# Create the master timestamp skeleton\n",
        "START_DT = datetime.fromtimestamp(START_MS / 1_000, tz=timezone.utc).replace(tzinfo=None)\n",
        "END_DT = datetime.fromtimestamp(END_MS / 1_000, tz=timezone.utc).replace(tzinfo=None)\n",
        "\n",
        "full_index = pl.datetime_range(\n",
        "    START_DT,\n",
        "    END_DT,\n",
        "    interval=INTERVAL,\n",
        "    time_unit=\"ms\",\n",
        "    eager=True,\n",
        ").alias(\"timestamp\")\n",
        "\n",
        "assert len(full_index) == EXPECTED_ROWS, f\"Index mismatch: {len(full_index)} vs {EXPECTED_ROWS}\"\n",
        "\n",
        "# --- Processing Loop ---\n",
        "parquet_files = sorted(DATA_DIR.glob(\"*.parquet\"))\n",
        "\n",
        "for path in parquet_files:\n",
        "    print(f\"Processing {path.name}...\")\n",
        "    \n",
        "    # 1. Load and Clean Raw Data\n",
        "    df_raw = (\n",
        "        pl.read_parquet(path)\n",
        "        .with_columns([\n",
        "            pl.col(\"timestamp\").cast(pl.Datetime(\"ms\")),\n",
        "            *[pl.col(c).cast(pl.Float64) for c in VALUE_COLS]\n",
        "        ])\n",
        "        # Remove duplicates if any exist in the raw source\n",
        "        .unique(subset=\"timestamp\", keep=\"first\")\n",
        "    )\n",
        "\n",
        "    # 2. Align to Master Index and Impute\n",
        "    # We use a 'left' join so every output file has the exact same 'full_index'\n",
        "    df_imputed = (\n",
        "        pl.DataFrame(full_index)\n",
        "        .join(df_raw, on=\"timestamp\", how=\"left\")\n",
        "        .with_columns([\n",
        "            # FORWARD FILL prices (use the last known price)\n",
        "            # BACKWARD FILL handles the case where the very first row of the file is missing\n",
        "            pl.col(PRICE_COLS).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\"),\n",
        "            \n",
        "            # ZERO FILL volume (no data usually means no trades)\n",
        "            pl.col(VOLUME_COL).fill_null(0.0)\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # 3. Final Format & Integrity Check\n",
        "    output = (\n",
        "        df_imputed\n",
        "        .with_columns(pl.col(\"timestamp\").dt.timestamp(\"ms\").cast(pl.Int64()))\n",
        "        .select([\"timestamp\", *VALUE_COLS])\n",
        "    )\n",
        "\n",
        "    if output.height != EXPECTED_ROWS:\n",
        "        raise ValueError(f\"Height mismatch for {path.name}: {output.height}\")\n",
        "\n",
        "    # 4. Save\n",
        "    out_path = OUT_DIR / path.name\n",
        "    output.write_parquet(out_path)\n",
        "    print(f\"Successfully saved to {out_path}\")\n",
        "\n",
        "print(\"\\nAll files processed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e735e484",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File                 | Imputed %  | Nulls  | Status\n",
            "-------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "# Ensure this matches the OUT_DIR from your processing script\n",
        "CLEANED_DIR = Path(\"data_cleaned\") \n",
        "EXPECTED_ROWS = 210_528\n",
        "\n",
        "# Verify directory exists and has files\n",
        "if not CLEANED_DIR.exists():\n",
        "    print(f\"Error: Directory '{CLEANED_DIR}' does not exist.\")\n",
        "    exit()\n",
        "\n",
        "parquet_files = list(CLEANED_DIR.glob(\"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    print(f\"Error: No parquet files found in '{CLEANED_DIR.absolute()}'.\")\n",
        "    exit()\n",
        "\n",
        "# --- Validation Table ---\n",
        "print(f\"{'File':<20} | {'Imputed %':<10} | {'Nulls':<6} | {'Status'}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for path in sorted(parquet_files):\n",
        "    # Use scan_parquet for faster metadata-only access where possible\n",
        "    df = pl.read_parquet(path)\n",
        "    \n",
        "    # Check 1: Missing Data (using volume as proxy)\n",
        "    imputed_count = df.filter(pl.col(\"volume\") == 0).height\n",
        "    imputed_pct = (imputed_count / EXPECTED_ROWS) * 100\n",
        "    \n",
        "    # Check 2: Total Nulls\n",
        "    null_count = df.null_count().sum_horizontal().item()\n",
        "    \n",
        "    # Check 3: Timestamp Gaps (Should be exactly 300,000ms)\n",
        "    # diff() results in a null for the first row, so we drop it\n",
        "    has_gaps = not (df[\"timestamp\"].diff().dropna() == 300_000).all()\n",
        "    \n",
        "    # Check 4: Correct Row Count\n",
        "    wrong_count = df.height != EXPECTED_ROWS\n",
        "    \n",
        "    status = \"OK\" if (null_count == 0 and not has_gaps and not wrong_count) else \"ERROR\"\n",
        "    \n",
        "    print(f\"{path.name:<20} | {imputed_pct:>9.2f}% | {null_count:>6} | {status}\")\n",
        "\n",
        "    if status == \"ERROR\":\n",
        "        if null_count > 0: print(f\"  --> [FAIL] {null_count} null values found.\")\n",
        "        if has_gaps:   print(f\"  --> [FAIL] Timestamp continuity broken.\")\n",
        "        if wrong_count: print(f\"  --> [FAIL] Expected {EXPECTED_ROWS} rows, got {df.height}.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
