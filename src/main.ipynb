{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "52b25605",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import kagglehub\n",
        "import polars as pl\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ac78d7",
      "metadata": {},
      "source": [
        "# Download Parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c6fbee06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching latest data from Kaggle: braydenmcarthur/10x-crypto-ohlcv-2024-2025...\n",
            "Saving 10 files to /home/zoltesh/projects/intelligent_diversification/src/data...\n",
            " -> Saved XRP-USDC.parquet\n",
            " -> Saved SOL-USDC.parquet\n",
            " -> Saved BCH-USDC.parquet\n",
            " -> Saved ADA-USDC.parquet\n",
            " -> Saved AVAX-USDC.parquet\n",
            " -> Saved ETH-USDC.parquet\n",
            " -> Saved BTC-USDC.parquet\n",
            " -> Saved LTC-USDC.parquet\n",
            " -> Saved DOGE-USDC.parquet\n",
            " -> Saved LINK-USDC.parquet\n",
            "\n",
            "Sync Complete. Files are located in: /home/zoltesh/projects/intelligent_diversification/src/data\n"
          ]
        }
      ],
      "source": [
        "def sync_and_save_parquet(dataset_slug: str, target_dirname: str = \"data\"):\n",
        "    # 1. Load credentials from .env\n",
        "    load_dotenv()\n",
        "    \n",
        "    if not os.getenv(\"KAGGLE_USERNAME\") or not os.getenv(\"KAGGLE_KEY\"):\n",
        "        raise EnvironmentError(\"KAGGLE_USERNAME or KAGGLE_KEY not found in .env file.\")\n",
        "\n",
        "    # 2. Define data path relative to where the code is running\n",
        "    # This works in Notebooks and .py scripts\n",
        "    project_root = Path.cwd()\n",
        "    local_data_dir = project_root / target_dirname\n",
        "    local_data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # 3. Download the dataset (returns path to kagglehub cache)\n",
        "    print(f\"Fetching latest data from Kaggle: {dataset_slug}...\")\n",
        "    cache_path = Path(kagglehub.dataset_download(dataset_slug))\n",
        "\n",
        "    # 4. Find all parquet files\n",
        "    parquet_files = list(cache_path.glob(\"*.parquet\"))\n",
        "    \n",
        "    if not parquet_files:\n",
        "        print(\"No parquet files found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # 5. Save to the local data folder\n",
        "    print(f\"Saving {len(parquet_files)} files to {local_data_dir}...\")\n",
        "    for file in parquet_files:\n",
        "        # Load and write using Polars\n",
        "        lf = pl.scan_parquet(file)\n",
        "        destination = local_data_dir / file.name\n",
        "        \n",
        "        # We use collect() here to pull the data from cache and write it locally\n",
        "        lf.collect().write_parquet(destination)\n",
        "        print(f\" -> Saved {file.name}\")\n",
        "\n",
        "    print(f\"\\nSync Complete. Files are located in: {local_data_dir}\")\n",
        "\n",
        "# Execution\n",
        "DATASET_SLUG = \"braydenmcarthur/10x-crypto-ohlcv-2024-2025\"\n",
        "\n",
        "try:\n",
        "    sync_and_save_parquet(DATASET_SLUG)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c4b083",
      "metadata": {},
      "source": [
        "# Check Total Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e3aca6bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of records: 2104131\n"
          ]
        }
      ],
      "source": [
        "num_records = 0\n",
        "for file in os.listdir('data/'):\n",
        "    num_records += pl.scan_parquet(f'data/{file}').collect().shape[0]\n",
        "print(f\"Total number of records: {num_records}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fa7f4cbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing ADA-USDC.parquet...\n",
            "Successfully saved to data_cleaned/ADA-USDC.parquet\n",
            "Processing AVAX-USDC.parquet...\n",
            "Successfully saved to data_cleaned/AVAX-USDC.parquet\n",
            "Processing BCH-USDC.parquet...\n",
            "Successfully saved to data_cleaned/BCH-USDC.parquet\n",
            "Processing BTC-USDC.parquet...\n",
            "Successfully saved to data_cleaned/BTC-USDC.parquet\n",
            "Processing DOGE-USDC.parquet...\n",
            "Successfully saved to data_cleaned/DOGE-USDC.parquet\n",
            "Processing ETH-USDC.parquet...\n",
            "Successfully saved to data_cleaned/ETH-USDC.parquet\n",
            "Processing LINK-USDC.parquet...\n",
            "Successfully saved to data_cleaned/LINK-USDC.parquet\n",
            "Processing LTC-USDC.parquet...\n",
            "Successfully saved to data_cleaned/LTC-USDC.parquet\n",
            "Processing SOL-USDC.parquet...\n",
            "Successfully saved to data_cleaned/SOL-USDC.parquet\n",
            "Processing XRP-USDC.parquet...\n",
            "Successfully saved to data_cleaned/XRP-USDC.parquet\n",
            "\n",
            "All files processed successfully.\n"
          ]
        }
      ],
      "source": [
        "# --- Path Resolution ---\n",
        "DATA_DIR = Path(\"data\")\n",
        "if not DATA_DIR.is_dir():\n",
        "    raise FileNotFoundError(\"Could not locate data directory.\")\n",
        "\n",
        "OUT_DIR = Path(\"data_cleaned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- Constants & Config ---\n",
        "START_MS = 1_704_067_200_000  # 2024-01-01 00:00:00 UTC\n",
        "END_MS = 1_767_225_300_000    # 2025-12-31 23:55:00 UTC\n",
        "INTERVAL = \"5m\"\n",
        "EXPECTED_ROWS = 210_528\n",
        "PRICE_COLS = [\"open\", \"high\", \"low\", \"close\"]\n",
        "VOLUME_COL = [\"volume\"]\n",
        "VALUE_COLS = PRICE_COLS + VOLUME_COL\n",
        "\n",
        "# Create the master timestamp skeleton\n",
        "START_DT = datetime.fromtimestamp(START_MS / 1_000, tz=timezone.utc).replace(tzinfo=None)\n",
        "END_DT = datetime.fromtimestamp(END_MS / 1_000, tz=timezone.utc).replace(tzinfo=None)\n",
        "\n",
        "full_index = pl.datetime_range(\n",
        "    START_DT,\n",
        "    END_DT,\n",
        "    interval=INTERVAL,\n",
        "    time_unit=\"ms\",\n",
        "    eager=True,\n",
        ").alias(\"timestamp\")\n",
        "\n",
        "assert len(full_index) == EXPECTED_ROWS, f\"Index mismatch: {len(full_index)} vs {EXPECTED_ROWS}\"\n",
        "\n",
        "# --- Processing Loop ---\n",
        "parquet_files = sorted(DATA_DIR.glob(\"*.parquet\"))\n",
        "\n",
        "for path in parquet_files:\n",
        "    print(f\"Processing {path.name}...\")\n",
        "    \n",
        "    # 1. Load and Clean Raw Data\n",
        "    df_raw = (\n",
        "        pl.read_parquet(path)\n",
        "        .with_columns([\n",
        "            pl.col(\"timestamp\").cast(pl.Datetime(\"ms\")),\n",
        "            *[pl.col(c).cast(pl.Float64) for c in VALUE_COLS]\n",
        "        ])\n",
        "        # Remove duplicates if any exist in the raw source\n",
        "        .unique(subset=\"timestamp\", keep=\"first\")\n",
        "    )\n",
        "\n",
        "    # 2. Align to Master Index and Impute\n",
        "    # We use a 'left' join so every output file has the exact same 'full_index'\n",
        "    df_imputed = (\n",
        "        pl.DataFrame(full_index)\n",
        "        .join(df_raw, on=\"timestamp\", how=\"left\")\n",
        "        .with_columns([\n",
        "            # FORWARD FILL prices (use the last known price)\n",
        "            # BACKWARD FILL handles the case where the very first row of the file is missing\n",
        "            pl.col(PRICE_COLS).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\"),\n",
        "            \n",
        "            # ZERO FILL volume (no data usually means no trades)\n",
        "            pl.col(VOLUME_COL).fill_null(0.0)\n",
        "        ])\n",
        "    )\n",
        "\n",
        "    # 3. Final Format & Integrity Check\n",
        "    output = (\n",
        "        df_imputed\n",
        "        .with_columns(pl.col(\"timestamp\").dt.timestamp(\"ms\").cast(pl.Int64()))\n",
        "        .select([\"timestamp\", *VALUE_COLS])\n",
        "    )\n",
        "\n",
        "    if output.height != EXPECTED_ROWS:\n",
        "        raise ValueError(f\"Height mismatch for {path.name}: {output.height}\")\n",
        "\n",
        "    # 4. Save\n",
        "    out_path = OUT_DIR / path.name\n",
        "    output.write_parquet(out_path)\n",
        "    print(f\"Successfully saved to {out_path}\")\n",
        "\n",
        "print(\"\\nAll files processed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e735e484",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File                 | Imputed %  | Nulls  | Status\n",
            "-------------------------------------------------------\n",
            "ADA-USDC.parquet     |      0.10% |      0 | OK\n",
            "AVAX-USDC.parquet    |      0.12% |      0 | OK\n",
            "BCH-USDC.parquet     |      0.10% |      0 | OK\n",
            "BTC-USDC.parquet     |      0.08% |      0 | OK\n",
            "DOGE-USDC.parquet    |      0.09% |      0 | OK\n",
            "ETH-USDC.parquet     |      0.09% |      0 | OK\n",
            "LINK-USDC.parquet    |      0.09% |      0 | OK\n",
            "LTC-USDC.parquet     |      0.09% |      0 | OK\n",
            "SOL-USDC.parquet     |      0.09% |      0 | OK\n",
            "XRP-USDC.parquet     |      0.09% |      0 | OK\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration ---\n",
        "CLEANED_DIR = Path(\"data_cleaned\") \n",
        "EXPECTED_ROWS = 210_528\n",
        "\n",
        "# Verify directory exists and has files\n",
        "if not CLEANED_DIR.exists():\n",
        "    print(f\"Error: Directory '{CLEANED_DIR}' does not exist.\")\n",
        "    exit()\n",
        "\n",
        "parquet_files = list(CLEANED_DIR.glob(\"*.parquet\"))\n",
        "if not parquet_files:\n",
        "    print(f\"Error: No parquet files found in '{CLEANED_DIR.absolute()}'.\")\n",
        "    exit()\n",
        "\n",
        "# --- Validation Table ---\n",
        "print(f\"{'File':<20} | {'Imputed %':<10} | {'Nulls':<6} | {'Status'}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for path in sorted(parquet_files):\n",
        "    # Use scan_parquet for faster metadata-only access where possible\n",
        "    df = pl.read_parquet(path)\n",
        "    \n",
        "    # Check 1: Missing Data (using volume as proxy)\n",
        "    imputed_count = df.filter(pl.col(\"volume\") == 0).height\n",
        "    imputed_pct = (imputed_count / EXPECTED_ROWS) * 100\n",
        "    \n",
        "    # Check 2: Total Nulls\n",
        "    null_count = df.null_count().sum_horizontal().item()\n",
        "    \n",
        "    # Check 3: Timestamp Gaps (Should be exactly 300,000ms)\n",
        "    # diff() results in a null for the first row, so we drop it\n",
        "    has_gaps = not (df[\"timestamp\"].diff().drop_nulls() == 300_000).all()\n",
        "    \n",
        "    # Check 4: Correct Row Count\n",
        "    wrong_count = df.height != EXPECTED_ROWS\n",
        "    \n",
        "    status = \"OK\" if (null_count == 0 and not has_gaps and not wrong_count) else \"ERROR\"\n",
        "    \n",
        "    print(f\"{path.name:<20} | {imputed_pct:>9.2f}% | {null_count:>6} | {status}\")\n",
        "\n",
        "    if status == \"ERROR\":\n",
        "        if null_count > 0: print(f\"  --> [FAIL] {null_count} null values found.\")\n",
        "        if has_gaps:   print(f\"  --> [FAIL] Timestamp continuity broken.\")\n",
        "        if wrong_count: print(f\"  --> [FAIL] Expected {EXPECTED_ROWS} rows, got {df.height}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582b1c57",
      "metadata": {},
      "source": [
        "# Technical Indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "27341754",
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_map = {\n",
        "    '5m': 5,\n",
        "    '15m': 15,\n",
        "    '30m': 30,\n",
        "    '1h': 60\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6050b7f5",
      "metadata": {},
      "source": [
        "## RSI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4cd39e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_rsi(df: pl.DataFrame, tf: str, period: int) -> pl.DataFrame:\n",
        "    # 1. Validation (Assuming tf_map is available or passed in)\n",
        "    tf_map = {'5m': 5, '15m': 15, '1h': 60} # Example local definition\n",
        "    minutes = tf_map.get(tf)\n",
        "    \n",
        "    if minutes is None:\n",
        "        raise ValueError(f\"Unsupported timeframe '{tf}'\")\n",
        "    if period <= 0:\n",
        "        raise ValueError(\"period must be a positive integer\")\n",
        "\n",
        "    # 2. Calculate Changes\n",
        "    change = pl.col(\"close\").diff()\n",
        "    \n",
        "    # 3. Define Gains and Losses\n",
        "    # We use abs() for losses to keep them positive for the math\n",
        "    gains = pl.when(change > 0).then(change).otherwise(0.0)\n",
        "    losses = pl.when(change < 0).then(change.abs()).otherwise(0.0)\n",
        "\n",
        "    # 4. Calculate Wilder's Smoothing (alpha = 1/N)\n",
        "    alpha = 1 / float(period)\n",
        "    avg_gain = gains.ewm_mean(alpha=alpha, adjust=False, min_samples=period)\n",
        "    avg_loss = losses.ewm_mean(alpha=alpha, adjust=False, min_samples=period)\n",
        "\n",
        "    # 5. Calculate RSI with Zero-Division Protection\n",
        "    # If avg_loss is 0, RS is infinite -> RSI is 100.\n",
        "    rs = avg_gain / avg_loss\n",
        "    \n",
        "    rsi_expr = pl.when(avg_loss == 0)\\\n",
        "        .then(100.0)\\\n",
        "        .otherwise(100.0 - (100.0 / (1.0 + rs)))\n",
        "\n",
        "    col_name = f\"rsi_{tf}_{period}\"\n",
        "    return df.with_columns(rsi_expr.alias(col_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b1c9a5a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = add_rsi(df=df, tf='5m', period=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5c38e1f4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>open</th><th>high</th><th>low</th><th>close</th><th>volume</th><th>rsi_5m_14</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1767224100000</td><td>1.8413</td><td>1.8414</td><td>1.8396</td><td>1.8403</td><td>296177.813697</td><td>57.750103</td></tr><tr><td>1767224400000</td><td>1.8403</td><td>1.8407</td><td>1.8387</td><td>1.8397</td><td>331597.905883</td><td>56.018254</td></tr><tr><td>1767224700000</td><td>1.8397</td><td>1.8398</td><td>1.838</td><td>1.838</td><td>317690.771665</td><td>51.322082</td></tr><tr><td>1767225000000</td><td>1.838</td><td>1.8398</td><td>1.8376</td><td>1.8398</td><td>311577.070954</td><td>55.569311</td></tr><tr><td>1767225300000</td><td>1.8398</td><td>1.8403</td><td>1.8389</td><td>1.839</td><td>171277.063977</td><td>53.341684</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 7)\n",
              "┌───────────────┬────────┬────────┬────────┬────────┬───────────────┬───────────┐\n",
              "│ timestamp     ┆ open   ┆ high   ┆ low    ┆ close  ┆ volume        ┆ rsi_5m_14 │\n",
              "│ ---           ┆ ---    ┆ ---    ┆ ---    ┆ ---    ┆ ---           ┆ ---       │\n",
              "│ i64           ┆ f64    ┆ f64    ┆ f64    ┆ f64    ┆ f64           ┆ f64       │\n",
              "╞═══════════════╪════════╪════════╪════════╪════════╪═══════════════╪═══════════╡\n",
              "│ 1767224100000 ┆ 1.8413 ┆ 1.8414 ┆ 1.8396 ┆ 1.8403 ┆ 296177.813697 ┆ 57.750103 │\n",
              "│ 1767224400000 ┆ 1.8403 ┆ 1.8407 ┆ 1.8387 ┆ 1.8397 ┆ 331597.905883 ┆ 56.018254 │\n",
              "│ 1767224700000 ┆ 1.8397 ┆ 1.8398 ┆ 1.838  ┆ 1.838  ┆ 317690.771665 ┆ 51.322082 │\n",
              "│ 1767225000000 ┆ 1.838  ┆ 1.8398 ┆ 1.8376 ┆ 1.8398 ┆ 311577.070954 ┆ 55.569311 │\n",
              "│ 1767225300000 ┆ 1.8398 ┆ 1.8403 ┆ 1.8389 ┆ 1.839  ┆ 171277.063977 ┆ 53.341684 │\n",
              "└───────────────┴────────┴────────┴────────┴────────┴───────────────┴───────────┘"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df.tail()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
