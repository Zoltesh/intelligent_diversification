{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "52b25605",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zoltesh/projects/intelligent_diversification/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import kagglehub\n",
        "import polars as pl\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266442f3",
      "metadata": {},
      "source": [
        "# Cleaning and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ac78d7",
      "metadata": {},
      "source": [
        "### Download Parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6fbee06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching latest data from Kaggle: braydenmcarthur/10x-crypto-ohlcv-2024-2025...\n",
            "Downloading to /home/zoltesh/.cache/kagglehub/datasets/braydenmcarthur/10x-crypto-ohlcv-2024-2025/3.archive...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 38.7M/38.7M [00:01<00:00, 39.6MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n",
            "Syncing 10 files to /home/zoltesh/projects/intelligent_diversification/src/data...\n",
            " -> Updated DOGE-USDC.parquet\n",
            " -> Updated XRP-USDC.parquet\n",
            " -> Updated BTC-USDC.parquet\n",
            " -> Updated BCH-USDC.parquet\n",
            " -> Updated LINK-USDC.parquet\n",
            " -> Updated ADA-USDC.parquet\n",
            " -> Updated LTC-USDC.parquet\n",
            " -> Updated ETH-USDC.parquet\n",
            " -> Updated SOL-USDC.parquet\n",
            " -> Updated AVAX-USDC.parquet\n",
            "\n",
            "Sync Complete. Files are located in: /home/zoltesh/projects/intelligent_diversification/src/data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def sync_and_save_parquet(dataset_slug: str, target_dirname: str = \"data\"):\n",
        "    # 1. Load credentials from .env\n",
        "    load_dotenv()\n",
        "    \n",
        "    if not os.getenv(\"KAGGLE_USERNAME\") or not os.getenv(\"KAGGLE_KEY\"):\n",
        "        raise EnvironmentError(\"KAGGLE_USERNAME or KAGGLE_KEY not found in .env file.\")\n",
        "\n",
        "    # 2. Define data path relative to where the code is running\n",
        "    project_root = Path.cwd()\n",
        "    local_data_dir = project_root / target_dirname\n",
        "    local_data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    # 3. Download the dataset (Bypassing local cache)\n",
        "    print(f\"Fetching latest data from Kaggle: {dataset_slug}...\")\n",
        "    # force_download=True ensures kagglehub checks for the latest version and re-downloads\n",
        "    cache_path = Path(kagglehub.dataset_download(dataset_slug, force_download=True))\n",
        "\n",
        "    # 4. Find all parquet files in the fresh download\n",
        "    parquet_files = list(cache_path.glob(\"*.parquet\"))\n",
        "    \n",
        "    if not parquet_files:\n",
        "        print(\"No parquet files found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # 5. Move/Save to the local data folder\n",
        "    print(f\"Syncing {len(parquet_files)} files to {local_data_dir}...\")\n",
        "    for file in parquet_files:\n",
        "        destination = local_data_dir / file.name\n",
        "        \n",
        "        # Using shutil.copy2 to preserve metadata and save CPU/Memory \n",
        "        # instead of reading/writing via Polars\n",
        "        shutil.copy2(file, destination)\n",
        "        print(f\" -> Updated {file.name}\")\n",
        "\n",
        "    print(f\"\\nSync Complete. Files are located in: {local_data_dir}\")\n",
        "\n",
        "# Execution\n",
        "# Using your specific dataset slug\n",
        "DATASET_SLUG = \"braydenmcarthur/10x-crypto-ohlcv-2024-2025\"\n",
        "\n",
        "try:\n",
        "    sync_and_save_parquet(DATASET_SLUG)\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169c3879",
      "metadata": {},
      "source": [
        "### Create the lazyframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c5fe6f89",
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_lf = pl.scan_parquet('data/ADA-USDC.parquet')\n",
        "avax_lf = pl.scan_parquet('data/AVAX-USDC.parquet')\n",
        "bch_lf = pl.scan_parquet('data/BCH-USDC.parquet')\n",
        "btc_lf = pl.scan_parquet('data/BTC-USDC.parquet')\n",
        "doge_lf = pl.scan_parquet('data/DOGE-USDC.parquet')\n",
        "eth_lf = pl.scan_parquet('data/ETH-USDC.parquet')\n",
        "link_lf = pl.scan_parquet('data/LINK-USDC.parquet')\n",
        "ltc_lf = pl.scan_parquet('data/LTC-USDC.parquet')\n",
        "sol_lf = pl.scan_parquet('data/SOL-USDC.parquet')\n",
        "xrp_lf = pl.scan_parquet('data/XRP-USDC.parquet')\n",
        "\n",
        "all_lf = {\n",
        "    'ADA': ada_lf,\n",
        "    'AVAX': avax_lf,\n",
        "    'BCH': bch_lf,\n",
        "    'BTC': btc_lf,\n",
        "    'DOGE': doge_lf,\n",
        "    'ETH': eth_lf,\n",
        "    'LINK': link_lf,\n",
        "    'LTC': ltc_lf,\n",
        "    'SOL': sol_lf,\n",
        "    'XRP': xrp_lf\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c4b083",
      "metadata": {},
      "source": [
        "### Identify sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e3aca6bf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA: 0.06% missing\n",
            "AVAX: 0.08% missing\n",
            "BCH: 0.06% missing\n",
            "BTC: 0.04% missing\n",
            "DOGE: 0.05% missing\n",
            "ETH: 0.05% missing\n",
            "LINK: 0.05% missing\n",
            "LTC: 0.05% missing\n",
            "SOL: 0.05% missing\n",
            "XRP: 0.05% missing\n",
            "Total sparsity: 0.05%\n"
          ]
        }
      ],
      "source": [
        "# Print percentage missing out of 210,528 rows per file\n",
        "# Print percentage missing out of total rows\n",
        "total_rows = 0\n",
        "total_missing = 0\n",
        "for symbol, lf in all_lf.items():\n",
        "    missing = (210_528 - lf.collect().shape[0]) / 210_528 * 100\n",
        "    total_missing += missing\n",
        "    print(f\"{symbol}: {missing:.2f}% missing\")\n",
        "    total_rows += lf.collect().shape[0]\n",
        "\n",
        "total_sparsity = (2_105_280 - total_rows) / 2_105_280 * 100\n",
        "print(f\"Total sparsity: {total_sparsity:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7ea3fc37",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2104132\n"
          ]
        }
      ],
      "source": [
        "total = 0\n",
        "for symbol, lf in all_lf.items():\n",
        "    total += lf.collect().shape[0]\n",
        "print(total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0127abcc",
      "metadata": {},
      "source": [
        "# Detect any gaps/missing timestamp and forward fill them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fa7f4cbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA: imputed 121 rows -> data_cleaned/ADA-USDC.parquet\n",
            "AVAX: imputed 161 rows -> data_cleaned/AVAX-USDC.parquet\n",
            "BCH: imputed 129 rows -> data_cleaned/BCH-USDC.parquet\n",
            "BTC: imputed 94 rows -> data_cleaned/BTC-USDC.parquet\n",
            "DOGE: imputed 105 rows -> data_cleaned/DOGE-USDC.parquet\n",
            "ETH: imputed 101 rows -> data_cleaned/ETH-USDC.parquet\n",
            "LINK: imputed 106 rows -> data_cleaned/LINK-USDC.parquet\n",
            "LTC: imputed 107 rows -> data_cleaned/LTC-USDC.parquet\n",
            "SOL: imputed 112 rows -> data_cleaned/SOL-USDC.parquet\n",
            "XRP: imputed 112 rows -> data_cleaned/XRP-USDC.parquet\n"
          ]
        }
      ],
      "source": [
        "OUT_DIR = Path(\"data_cleaned\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "INTERVAL = \"5m\"\n",
        "PRICE_COLS = [\"open\", \"high\", \"low\", \"close\"]\n",
        "VOLUME_COLS = [\"volume\"]\n",
        "VALUE_COLS = PRICE_COLS + VOLUME_COLS\n",
        "\n",
        "def impute_gaps_ffill(lf: pl.LazyFrame, every: str = \"5m\") -> tuple[pl.LazyFrame, int]:\n",
        "    base = (\n",
        "        lf.select([\"timestamp\", *VALUE_COLS])\n",
        "          .with_columns(\n",
        "              pl.col(\"timestamp\").cast(pl.Datetime(\"ms\")),\n",
        "              *[pl.col(c).cast(pl.Float64) for c in VALUE_COLS],\n",
        "          )\n",
        "          .unique(subset=\"timestamp\", keep=\"first\")\n",
        "          .sort(\"timestamp\")\n",
        "    )\n",
        "\n",
        "    # derive start/end from the data (small collect)\n",
        "    bounds = base.select(\n",
        "        pl.col(\"timestamp\").min().alias(\"start\"),\n",
        "        pl.col(\"timestamp\").max().alias(\"end\"),\n",
        "    ).collect()\n",
        "    start = bounds[\"start\"][0]\n",
        "    end = bounds[\"end\"][0]\n",
        "\n",
        "    full_index = pl.datetime_range(\n",
        "        start, end, interval=every, time_unit=\"ms\", eager=True\n",
        "    )\n",
        "\n",
        "    skeleton = pl.DataFrame({\"timestamp\": full_index}).lazy()\n",
        "\n",
        "    joined = skeleton.join(base, on=\"timestamp\", how=\"left\")\n",
        "\n",
        "    imputed_count = int(\n",
        "        joined.select(pl.col(\"open\").is_null().sum()).collect().item()\n",
        "    )\n",
        "\n",
        "    out = (\n",
        "        joined.with_columns(\n",
        "            pl.col(VALUE_COLS).fill_null(strategy=\"forward\")  # includes volume now\n",
        "        )\n",
        "        .with_columns(pl.col(\"timestamp\").dt.timestamp(\"ms\").cast(pl.Int64()))\n",
        "        .select([\"timestamp\", *VALUE_COLS])\n",
        "    )\n",
        "\n",
        "    return out, imputed_count\n",
        "\n",
        "for symbol, lf in all_lf.items():\n",
        "    out_lf, imputed = impute_gaps_ffill(lf, every=INTERVAL)\n",
        "    out_path = OUT_DIR / f\"{symbol}-USDC.parquet\"\n",
        "    out_lf.sink_parquet(out_path)\n",
        "    print(f\"{symbol}: imputed {imputed} rows -> {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e735e484",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File                 |     Rows | DupTS | Gaps | Nulls | Status\n",
            "----------------------------------------------------------------------\n",
            "ADA-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "AVAX-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "BCH-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "BTC-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "DOGE-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "ETH-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "LINK-USDC.parquet    |   210528 | False | False |     0 | OK\n",
            "LTC-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "SOL-USDC.parquet     |   210528 | False | False |     0 | OK\n",
            "XRP-USDC.parquet     |   210528 | False | False |     0 | OK\n"
          ]
        }
      ],
      "source": [
        "CLEANED_DIR = Path(\"data_cleaned\")\n",
        "STEP_MS = 300_000  # 5 minutes\n",
        "\n",
        "print(f\"{'File':<20} | {'Rows':>8} | {'DupTS':>5} | {'Gaps':>4} | {'Nulls':>5} | {'Status'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for path in sorted(CLEANED_DIR.glob(\"*.parquet\")):\n",
        "    df = pl.read_parquet(path).select([\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]).sort(\"timestamp\")\n",
        "\n",
        "    ts = df[\"timestamp\"]\n",
        "    d = ts.diff().drop_nulls()\n",
        "\n",
        "    has_gaps = not (d == STEP_MS).all()\n",
        "    has_dups = ts.n_unique() != ts.len()\n",
        "\n",
        "    # expected row count for a perfectly continuous series over [min, max]\n",
        "    expected_rows = int((ts.max() - ts.min()) // STEP_MS + 1)\n",
        "    wrong_count = df.height != expected_rows\n",
        "\n",
        "    nulls = int(df.null_count().sum_horizontal().item())\n",
        "\n",
        "    status = \"OK\" if (nulls == 0 and not has_gaps and not has_dups and not wrong_count) else \"ERROR\"\n",
        "    print(f\"{path.name:<20} | {df.height:>8} | {str(has_dups):>5} | {str(has_gaps):>4} | {nulls:>5} | {status}\")\n",
        "\n",
        "    if status == \"ERROR\":\n",
        "        # show the first few offending deltas\n",
        "        bad = df.select(\n",
        "            pl.col(\"timestamp\"),\n",
        "            pl.col(\"timestamp\").diff().alias(\"dt\"),\n",
        "        ).filter(pl.col(\"dt\").is_not_null() & (pl.col(\"dt\") != STEP_MS)).head(10)\n",
        "        if bad.height:\n",
        "            print(bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc0dd60",
      "metadata": {},
      "source": [
        "# Create Cleaned LazyFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3217ae6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "ada_lf_cleaned = pl.scan_parquet('data_cleaned/ADA-USDC.parquet')\n",
        "avax_lf_cleaned = pl.scan_parquet('data_cleaned/AVAX-USDC.parquet')\n",
        "bch_lf_cleaned = pl.scan_parquet('data_cleaned/BCH-USDC.parquet')\n",
        "btc_lf_cleaned = pl.scan_parquet('data_cleaned/BTC-USDC.parquet')\n",
        "doge_lf_cleaned = pl.scan_parquet('data_cleaned/DOGE-USDC.parquet')\n",
        "eth_lf_cleaned = pl.scan_parquet('data_cleaned/ETH-USDC.parquet')\n",
        "link_lf_cleaned = pl.scan_parquet('data_cleaned/LINK-USDC.parquet')\n",
        "ltc_lf_cleaned = pl.scan_parquet('data_cleaned/LTC-USDC.parquet')\n",
        "sol_lf_cleaned = pl.scan_parquet('data_cleaned/SOL-USDC.parquet')\n",
        "xrp_lf_cleaned = pl.scan_parquet('data_cleaned/XRP-USDC.parquet')\n",
        "\n",
        "all_lf_cleaned = {\n",
        "    'ADA': ada_lf_cleaned,\n",
        "    'AVAX': avax_lf_cleaned,\n",
        "    'BCH': bch_lf_cleaned,\n",
        "    'BTC': btc_lf_cleaned,\n",
        "    'DOGE': doge_lf_cleaned,\n",
        "    'ETH': eth_lf_cleaned,\n",
        "    'LINK': link_lf_cleaned,\n",
        "    'LTC': ltc_lf_cleaned,\n",
        "    'SOL': sol_lf_cleaned,\n",
        "    'XRP': xrp_lf_cleaned\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "346cf989",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADA 1704067200000 1767225300000\n",
            "AVAX 1704067200000 1767225300000\n",
            "BCH 1704067200000 1767225300000\n",
            "BTC 1704067200000 1767225300000\n",
            "DOGE 1704067200000 1767225300000\n",
            "ETH 1704067200000 1767225300000\n",
            "LINK 1704067200000 1767225300000\n",
            "LTC 1704067200000 1767225300000\n",
            "SOL 1704067200000 1767225300000\n",
            "XRP 1704067200000 1767225300000\n"
          ]
        }
      ],
      "source": [
        "for symbol, lf in all_lf_cleaned.items():\n",
        "    print(f'{symbol} {lf.collect()['timestamp'].min()} {lf.collect()['timestamp'].max()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582b1c57",
      "metadata": {},
      "source": [
        "# Technical Indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "27341754",
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_map = {\n",
        "    '5m': 5,\n",
        "    '15m': 15,\n",
        "    '30m': 30,\n",
        "    '1h': 60\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6050b7f5",
      "metadata": {},
      "source": [
        "### RSI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb4cd39e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_rsi(df: pl.DataFrame, tf: str, period: int) -> pl.DataFrame:\n",
        "    # 1. Validation (Assuming tf_map is available or passed in)\n",
        "    tf_map = {'5m': 5, '15m': 15, '1h': 60} # Example local definition\n",
        "    minutes = tf_map.get(tf)\n",
        "    \n",
        "    if minutes is None:\n",
        "        raise ValueError(f\"Unsupported timeframe '{tf}'\")\n",
        "    if period <= 0:\n",
        "        raise ValueError(\"period must be a positive integer\")\n",
        "\n",
        "    # 2. Calculate Changes\n",
        "    change = pl.col(\"close\").diff()\n",
        "    \n",
        "    # 3. Define Gains and Losses\n",
        "    # We use abs() for losses to keep them positive for the math\n",
        "    gains = pl.when(change > 0).then(change).otherwise(0.0)\n",
        "    losses = pl.when(change < 0).then(change.abs()).otherwise(0.0)\n",
        "\n",
        "    # 4. Calculate Wilder's Smoothing (alpha = 1/N)\n",
        "    alpha = 1 / float(period)\n",
        "    avg_gain = gains.ewm_mean(alpha=alpha, adjust=False, min_samples=period)\n",
        "    avg_loss = losses.ewm_mean(alpha=alpha, adjust=False, min_samples=period)\n",
        "\n",
        "    # 5. Calculate RSI with Zero-Division Protection\n",
        "    # If avg_loss is 0, RS is infinite -> RSI is 100.\n",
        "    rs = avg_gain / avg_loss\n",
        "    \n",
        "    rsi_expr = pl.when(avg_loss == 0)\\\n",
        "        .then(100.0)\\\n",
        "        .otherwise(100.0 - (100.0 / (1.0 + rs)))\n",
        "\n",
        "    col_name = f\"rsi_{tf}_{period}\"\n",
        "    return df.with_columns(rsi_expr.alias(col_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b1c9a5a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = add_rsi(df=df, tf='5m', period=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5c38e1f4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>open</th><th>high</th><th>low</th><th>close</th><th>volume</th><th>rsi_5m_14</th></tr><tr><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1767224100000</td><td>1.8413</td><td>1.8414</td><td>1.8396</td><td>1.8403</td><td>296177.813697</td><td>57.750103</td></tr><tr><td>1767224400000</td><td>1.8403</td><td>1.8407</td><td>1.8387</td><td>1.8397</td><td>331597.905883</td><td>56.018254</td></tr><tr><td>1767224700000</td><td>1.8397</td><td>1.8398</td><td>1.838</td><td>1.838</td><td>317690.771665</td><td>51.322082</td></tr><tr><td>1767225000000</td><td>1.838</td><td>1.8398</td><td>1.8376</td><td>1.8398</td><td>311577.070954</td><td>55.569311</td></tr><tr><td>1767225300000</td><td>1.8398</td><td>1.8403</td><td>1.8389</td><td>1.839</td><td>171277.063977</td><td>53.341684</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 7)\n",
              "┌───────────────┬────────┬────────┬────────┬────────┬───────────────┬───────────┐\n",
              "│ timestamp     ┆ open   ┆ high   ┆ low    ┆ close  ┆ volume        ┆ rsi_5m_14 │\n",
              "│ ---           ┆ ---    ┆ ---    ┆ ---    ┆ ---    ┆ ---           ┆ ---       │\n",
              "│ i64           ┆ f64    ┆ f64    ┆ f64    ┆ f64    ┆ f64           ┆ f64       │\n",
              "╞═══════════════╪════════╪════════╪════════╪════════╪═══════════════╪═══════════╡\n",
              "│ 1767224100000 ┆ 1.8413 ┆ 1.8414 ┆ 1.8396 ┆ 1.8403 ┆ 296177.813697 ┆ 57.750103 │\n",
              "│ 1767224400000 ┆ 1.8403 ┆ 1.8407 ┆ 1.8387 ┆ 1.8397 ┆ 331597.905883 ┆ 56.018254 │\n",
              "│ 1767224700000 ┆ 1.8397 ┆ 1.8398 ┆ 1.838  ┆ 1.838  ┆ 317690.771665 ┆ 51.322082 │\n",
              "│ 1767225000000 ┆ 1.838  ┆ 1.8398 ┆ 1.8376 ┆ 1.8398 ┆ 311577.070954 ┆ 55.569311 │\n",
              "│ 1767225300000 ┆ 1.8398 ┆ 1.8403 ┆ 1.8389 ┆ 1.839  ┆ 171277.063977 ┆ 53.341684 │\n",
              "└───────────────┴────────┴────────┴────────┴────────┴───────────────┴───────────┘"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_df.tail()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
